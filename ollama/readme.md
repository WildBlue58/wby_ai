# Ollama

Ollama æ˜¯ä¸€ä¸ªè®©ä½ é€šè¿‡ç®€å•å‘½ä»¤åœ¨æœ¬åœ°è½»æ¾ä¸‹è½½ã€è¿è¡Œå’Œç®¡ç†å¤§è¯­è¨€æ¨¡å‹çš„å·¥å…·ï¼Œæ”¯æŒGPUåŠ é€Ÿå’Œç±»OpenAIæ¥å£ï¼Œé€‚åˆæœ¬åœ°éƒ¨ç½²å’Œå¼€å‘ã€‚

## ä¸»è¦ç‰¹æ€§

- ğŸš€ **ç®€å•æ˜“ç”¨**ï¼šé€šè¿‡å‘½ä»¤è¡Œè½»æ¾ç®¡ç†AIæ¨¡å‹
- ğŸ–¥ï¸ **æœ¬åœ°éƒ¨ç½²**ï¼šå®Œå…¨æœ¬åœ°è¿è¡Œï¼Œä¿æŠ¤æ•°æ®éšç§
- âš¡ **GPUåŠ é€Ÿ**ï¼šæ”¯æŒNVIDIA GPUåŠ é€Ÿæ¨ç†
- ğŸ”Œ **APIå…¼å®¹**ï¼šæä¾›ç±»ä¼¼OpenAIçš„APIæ¥å£
- ğŸ“¦ **æ¨¡å‹ç®¡ç†**ï¼šè‡ªåŠ¨ä¸‹è½½ã€æ›´æ–°å’Œç®¡ç†æ¨¡å‹

## æ”¯æŒçš„æ¨¡å‹

### Meta Llama ç³»åˆ—

- `llama3.2` - æœ€æ–°ç‰ˆæœ¬Llamaæ¨¡å‹
- `llama3.1` - ç¨³å®šç‰ˆæœ¬
- `llama3` - ç»å…¸ç‰ˆæœ¬

### DeepSeek ç³»åˆ—

- `deepseek-r1:1.5b` - 1.5Bå‚æ•°çš„æ¨ç†æ¨¡å‹
- `deepseek-coder` - ä»£ç ç”Ÿæˆä¸“ç”¨æ¨¡å‹

### Qwen ç³»åˆ—

- `qwen2.5` - é˜¿é‡Œå·´å·´å¼€æºæ¨¡å‹
- `qwen2.5-coder` - ä»£ç ä¸“ç”¨ç‰ˆæœ¬

## å®‰è£…å’Œä½¿ç”¨

### å®‰è£… Ollama

```bash
# è®¿é—®å®˜ç½‘ä¸‹è½½å®‰è£…åŒ…
https://ollama.ai/download
```

### åŸºæœ¬å‘½ä»¤

```bash
# æ‹‰å–æ¨¡å‹
ollama pull llama3.2

# è¿è¡Œæ¨¡å‹ï¼ˆäº¤äº’å¼ï¼‰
ollama run llama3.2

# è¿è¡Œæ¨¡å‹ï¼ˆä¸€æ¬¡æ€§å¯¹è¯ï¼‰
ollama run llama3.2 "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"
```

## API æœåŠ¡

Ollama åœ¨ `11434` ç«¯å£æä¾› REST API æœåŠ¡ï¼š

### å¯åŠ¨æœåŠ¡

```bash
ollama serve
```

### API è°ƒç”¨ç¤ºä¾‹

```bash
# ç”Ÿæˆæ–‡æœ¬
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±",
  "stream": false
}'

# èŠå¤©å¯¹è¯
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {
      "role": "user",
      "content": "ä½ å¥½"
    }
  ]
}'
```

## å¸¸ç”¨é…ç½®

### ç¯å¢ƒå˜é‡

```bash
# è®¾ç½®æ¨¡å‹å­˜å‚¨è·¯å¾„
export OLLAMA_MODELS=~/models

# è®¾ç½®APIç«¯å£ï¼ˆé»˜è®¤11434ï¼‰
export OLLAMA_HOST=0.0.0.0:11434
```

### é…ç½®æ–‡ä»¶

é…ç½®æ–‡ä»¶ä½ç½®ï¼š`~/.ollama/models/manifests/registry.ollama.ai/`

## æ€§èƒ½ä¼˜åŒ–

### GPU åŠ é€Ÿ

```bash
# æ£€æŸ¥GPUæ”¯æŒ
ollama list

# ä½¿ç”¨ç‰¹å®šGPU
CUDA_VISIBLE_DEVICES=0 ollama run llama3.2
```

### å†…å­˜ç®¡ç†

```bash
# æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯
ollama show llama3.2

# å¸è½½æ¨¡å‹é‡Šæ”¾å†…å­˜
ollama rm llama3.2
```

## å¼€å‘é›†æˆ

### Python å®¢æˆ·ç«¯

```python
import requests

response = requests.post('http://localhost:11434/api/generate',
                        json={
                            'model': 'llama3.2',
                            'prompt': 'ä½ å¥½',
                            'stream': False
                        })
print(response.json()['response'])
```

### JavaScript å®¢æˆ·ç«¯

```javascript
const response = await fetch('http://localhost:11434/api/generate', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
        model: 'llama3.2',
        prompt: 'ä½ å¥½',
        stream: false
    })
});
const data = await response.json();
console.log(data.response);
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **ç«¯å£è¢«å ç”¨**ï¼šä¿®æ”¹ `OLLAMA_HOST` ç¯å¢ƒå˜é‡
2. **å†…å­˜ä¸è¶³**ï¼šé€‰æ‹©æ›´å°çš„æ¨¡å‹æˆ–å¢åŠ è™šæ‹Ÿå†…å­˜
3. **GPUä¸æ”¯æŒ**ï¼šæ£€æŸ¥CUDAç‰ˆæœ¬å’Œé©±åŠ¨

### æ—¥å¿—æŸ¥çœ‹

```bash
# æŸ¥çœ‹æœåŠ¡æ—¥å¿—
ollama serve --verbose

# æŸ¥çœ‹æ¨¡å‹æ—¥å¿—
ollama logs <model_name>
```

## ç›¸å…³é“¾æ¥

- [å®˜æ–¹æ–‡æ¡£](https://ollama.ai/docs)
- [GitHubä»“åº“](https://github.com/ollama/ollama)
- [æ¨¡å‹åº“](https://ollama.ai/library)
- [ç¤¾åŒºè®ºå›](https://github.com/ollama/ollama/discussions)

## è®¸å¯è¯

Ollama é‡‡ç”¨ MIT è®¸å¯è¯ï¼Œè¯¦è§ [LICENSE](https://github.com/ollama/ollama/blob/main/LICENSE) æ–‡ä»¶ã€‚
